{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMmauXGh+Qj6dcfpEDOytDo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import random\n","from google.colab import drive\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import time\n","import copy\n","import numpy as np\n","\n","\n","def set_random_seeds(random_seed=0):\n","\n","    torch.manual_seed(random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(random_seed)\n","    random.seed(random_seed)\n","\n","\n","def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n","\n","    train_transform = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","    ])\n","    train_set = torchvision.datasets.CIFAR100(root=\"data\", train=True, download=True, transform=train_transform)\n","    test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=test_transform)\n","\n","    train_sampler = torch.utils.data.RandomSampler(train_set)\n","    test_sampler = torch.utils.data.SequentialSampler(test_set)\n","\n","    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, sampler=train_sampler, num_workers=num_workers)\n","\n","    test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=eval_batch_size, sampler=test_sampler, num_workers=num_workers)\n","    return train_loader, test_loader\n","\n","\n","def evaluate_model(model, test_loader, device, criterion=None):\n","\n","    model.eval()\n","    model.to(device)\n","\n","    running_loss = 0\n","    running_corrects = 0\n","\n","    for inputs, labels in test_loader:\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        if criterion is not None:\n","            loss = criterion(outputs, labels).item()\n","        else:\n","            loss = 0\n","\n","        # statistics\n","        running_loss += loss * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","\n","    eval_loss = running_loss / len(test_loader.dataset)\n","    eval_accuracy = running_corrects / len(test_loader.dataset)\n","\n","    return eval_loss, eval_accuracy\n","\n","\n","def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n","\n","    # The training configurations were not carefully selected.\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model.to(device)\n","\n","    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n","    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","\n","    # Evaluation\n","    model.eval()\n","    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n","    print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(0, eval_loss, eval_accuracy))\n","\n","    for epoch in range(num_epochs):\n","\n","        # Training\n","        model.train()\n","\n","        running_loss = 0\n","        running_corrects = 0\n","\n","        for inputs, labels in train_loader:\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_accuracy = running_corrects / len(train_loader.dataset)\n","\n","        # Evaluation\n","        model.eval()\n","        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n","\n","        # Set learning rate scheduler\n","        scheduler.step()\n","\n","        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n","            .format(epoch + 1, train_loss, train_accuracy, eval_loss, eval_accuracy))\n","\n","    return model\n","\n","\n","def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n","\n","    model.to(device)\n","    model.eval()\n","\n","    for inputs, labels in loader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        _ = model(inputs)\n","\n","\n","def measure_inference_latency(model, device, input_size=(1, 3, 32, 32), num_samples=100, num_warmups=10):\n","\n","    model.to(device)\n","    model.eval()\n","\n","    x = torch.rand(size=input_size).to(device)\n","\n","    with torch.no_grad():\n","        for _ in range(num_warmups):\n","            _ = model(x)\n","    torch.cuda.synchronize()\n","\n","    with torch.no_grad():\n","        start_time = time.time()\n","        for _ in range(num_samples):\n","            _ = model(x)\n","            torch.cuda.synchronize()\n","        end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    elapsed_time_ave = elapsed_time / num_samples\n","\n","    return elapsed_time_ave\n","\n","\n","def save_model(model, model_dir, model_filename):\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    model_filepath = os.path.join(model_dir, model_filename)\n","    torch.save(model.state_dict(), model_filepath)\n","\n","\n","def load_model(model, model_filepath, device):\n","\n","    model.load_state_dict(torch.load(model_filepath, map_location=device))\n","\n","    return model\n","\n","\n","def save_torchscript_model(model, model_dir, model_filename):\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    model_filepath = os.path.join(model_dir, model_filename)\n","    torch.jit.save(torch.jit.script(model), model_filepath)\n","\n","\n","def load_torchscript_model(model_filepath, device):\n","\n","    model = torch.jit.load(model_filepath, map_location=device)\n","\n","    return model\n","\n","\n","def create_model(num_classes=10):\n","\n","    # The number of channels in ResNet18 is divisible by 8.\n","    # This is required for fast GEMM integer matrix multiplication.\n","    # model = torchvision.models.resnet18(pretrained=False)\n","    model = resnet34(num_classes=num_classes, pretrained=False)\n","\n","    # We would use the pretrained ResNet18 as a feature extractor.\n","    # for param in model.parameters():\n","    #     param.requires_grad = False\n","\n","    # Modify the last FC layer\n","    # num_features = model.fc.in_features\n","    # model.fc = nn.Linear(num_features, 10)\n","\n","    return model\n","\n","\n","class QuantizedResNet18(nn.Module):\n","    def __init__(self, model_fp32):\n","\n","        super(QuantizedResNet18, self).__init__()\n","        # QuantStub converts tensors from floating point to quantized.\n","        # This will only be used for inputs.\n","        self.quant = torch.quantization.QuantStub()\n","        # DeQuantStub converts tensors from quantized to floating point.\n","        # This will only be used for outputs.\n","        self.dequant = torch.quantization.DeQuantStub()\n","        # FP32 model\n","        self.model_fp32 = model_fp32\n","\n","    def forward(self, x):\n","        # manually specify where tensors will be converted from floating\n","        # point to quantized in the quantized model\n","        x = self.quant(x)\n","        x = self.model_fp32(x)\n","        # manually specify where tensors will be converted from quantized\n","        # to floating point in the quantized model\n","        x = self.dequant(x)\n","        return x\n","\n","def model_running_process(weights_path, state_dict_path):\n","    random_seed = 0\n","    num_classes = 100\n","    cuda_device = torch.device(\"cuda:0\")\n","    cpu_device = torch.device(\"cpu:0\")\n","    set_random_seeds(random_seed=random_seed)\n","\n","    model = resnet34(num_classes=num_classes, pretrained=False) #change this line into your resnet code\n","\n","    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n","\n","    # Train model.\n","    print(\"Training Model...\")\n","    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)    #TESTING\n","    torch.save(model, weights_path)\n","    torch.save(model.state_dict(), state_dict_path)\n","    return model"],"metadata":{"id":"oWeM2oL5at-e"},"execution_count":null,"outputs":[]}]}